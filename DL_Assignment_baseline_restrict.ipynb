{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install scikit-image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xveQIshNWYdc"
   },
   "source": [
    "# 0. Loading Dataset\n",
    "\n",
    "You can download the dataset from {https://darwin.v7labs.com/v7-labs/covid-19-chest-x-ray-dataset?sort=priority\\%3Adesc}.\n",
    "The data entitled as '`darwin dataset pull v7-labs/covid-19-chest-x-ray-dataset:all-images`' will be used in this assignment. All dataset consist of 6504 images from 702 classes. We will extract the images of 4 classes (Bacterial Pneumonia, Viral Pneumonia, No Pneumonia (healthy), Covid-19) and save them as .npy file with the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "I6bqTVRSs-sw",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-04 09:01:57.911810: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-04 09:01:58.137589: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-03-04 09:01:58.137614: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-03-04 09:01:59.850707: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-03-04 09:01:59.850918: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-03-04 09:01:59.850928: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore')\n",
    "import keras\n",
    "import json\n",
    "import numpy as np\n",
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "import urllib.request\n",
    "from PIL import Image\n",
    "from keras import layers, models\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from skimage import io, color\n",
    "import random\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################\n",
    "# > DISABLED - ALREADY EXECUTED < #\n",
    "###################################\n",
    "\n",
    "\n",
    "# # all-images file should be uploaded to the same file\n",
    "# imageNames = glob.glob(\"all-images/*\")\n",
    "# \n",
    "# dataset = []\n",
    "# labels = []\n",
    "# \n",
    "# for i, imName in enumerate(imageNames):\n",
    "# \n",
    "#     # Opening JSON file\n",
    "#     f = open(imName)\n",
    "#     data = json.load(f)\n",
    "#     for j in range(len(data['annotations'])):\n",
    "# \n",
    "#         if 'COVID-19' in (data['annotations'][j]['name']):\n",
    "#           #load images from url    \n",
    "#             urllib.request.urlretrieve(data['image']['url'],\"img.png\")    \n",
    "#             img = Image.open(\"img.png\")\n",
    "#             #convert images to grayscale\n",
    "#             imgGray = img.convert('L')\n",
    "#             #resize the image (156x156)\n",
    "#             im = imgGray.resize((156,156), Image.LANCZOS)           \n",
    "#             label = data['annotations'][j]['name']\n",
    "#             dataset.append(np.array(im))\n",
    "#             labels.append(label)\n",
    "#             print(label)\n",
    "#             break\n",
    "# \n",
    "#         if 'Viral Pneumonia' in (data['annotations'][j]['name']) \\\n",
    "#             or 'Bacterial Pneumonia' in (data['annotations'][j]['name']) \\\n",
    "#             or 'No Pneumonia (healthy)' in (data['annotations'][j]['name']):\n",
    "#             #load images from url    \n",
    "#             urllib.request.urlretrieve(data['image']['url'],\"img.png\")    \n",
    "#             img = Image.open(\"img.png\")\n",
    "#             #convert images to grayscale\n",
    "#             imgGray = img.convert('L')\n",
    "#             #resize the image (156x156)\n",
    "#             im = imgGray.resize((156,156), Image.LANCZOS)           \n",
    "#             label = data['annotations'][j]['name']\n",
    "#             dataset.append(np.array(im))\n",
    "#             labels.append(label)\n",
    "#             break\n",
    "# \n",
    "# #Convert data shape of (n_of_samples, width, height, 1)\n",
    "# dataset = np.dstack(dataset)    \n",
    "# dataset = np.rollaxis(dataset,-1)\n",
    "# labels = np.array(labels)\n",
    "# \n",
    "# #convert images gray scale to rgb\n",
    "# data = np.array(layers.Lambda(tf.image.grayscale_to_rgb)(tf.expand_dims(dataset, -1)))\n",
    "# \n",
    "# # save data and labels into a folder\n",
    "# np.save(\"data.npy\", data)\n",
    "# np.save(\"labels.npy\", labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dn3sH7yJWboe"
   },
   "source": [
    "Once you save your data, you can load it from your directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "b6ZPDhVLWbyq"
   },
   "outputs": [],
   "source": [
    "features = np.load('data/data.npy')\n",
    "labels = np.load('data/labels.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B1b7dP-VbJpC"
   },
   "source": [
    "# 1. Data preprocessing\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Splitting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Note from Diego: discuss in the meeting \n",
    "what is exactly stratify and do we need it?\n",
    "'''\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    features, labels, test_size=0.2, random_state=42, stratify=labels)\n",
    "\n",
    "# Further split the training set into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train, y_train, test_size=0.25, random_state=42, stratify=y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4147\n",
      "1383\n",
      "1383\n"
     ]
    }
   ],
   "source": [
    "print(len(X_train))\n",
    "print(len(X_val))\n",
    "print(len(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra: Easy going here! - Sample restriction ðŸ˜ŽðŸ¤™ðŸŒ´  \\#prayforkernel ðŸ™"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "#             CONTROL PANEL\n",
    "\n",
    "# Sample restriction\n",
    "restriction = True\n",
    "proportion = 0.25\n",
    "#                                           \n",
    "#############################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_labels(data, labels):\n",
    "    \n",
    "    img_label_dict = {}\n",
    "    \n",
    "    # relate image and label index in dictionary\n",
    "    for i in range(len(labels)):\n",
    "        img_label_dict[i] = labels[i]\n",
    "        \n",
    "        \n",
    "    # Store all the indexes by label\n",
    "    BP_index = {k for k,v in img_label_dict.items() if v == 'Bacterial Pneumonia'}\n",
    "    VP_index = {k for k,v in img_label_dict.items() if v == 'Viral Pneumonia'}\n",
    "    NP_index = {k for k,v in img_label_dict.items() if v == 'No Pneumonia (healthy)'}\n",
    "    \n",
    "    CV_index = {k for k,v in img_label_dict.items() if v == 'COVID-19'}\n",
    "\n",
    "    return [BP_index, VP_index, NP_index, CV_index], img_label_dict\n",
    "\n",
    "\n",
    "def restrict_sample(proportion, data, labels):\n",
    "\n",
    "    classified_instances, img_label_dict = collect_labels(data, labels)\n",
    "\n",
    "    restricted_sample_indices = set()\n",
    "    restricted_img_label_dict = {}\n",
    "\n",
    "    for i in classified_instances[:-1]:\n",
    "        tmp = random.sample(i, k=int(len(i)*proportion))\n",
    "        for e in tmp:\n",
    "            restricted_sample_indices.add(e)\n",
    "\n",
    "    restricted_sample_indices = restricted_sample_indices.union(classified_instances[-1])\n",
    "    restricted_sample_labels = [img_label_dict.get(i) for i in restricted_sample_indices]\n",
    "    restricted_sample = [data[i] for i in restricted_sample_indices]\n",
    "\n",
    "    return restricted_sample, restricted_sample_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if restriction == True:\n",
    "\n",
    "    restricted_sample_train, restricted_sample_labels_train = restrict_sample(proportion , X_train, y_train)\n",
    "    restricted_sample_val, restricted_sample_labels_val = restrict_sample(proportion , X_val, y_val)\n",
    "    restricted_sample_test, restricted_sample_labels_test = restrict_sample(proportion , X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAMPLE RESTRICTION SUMMARY\n",
      "\n",
      "\n",
      "ORIGINAL TRAINING SAMPLE\n",
      "Original number of instances: 4147\n",
      "Original instance distribution by class: \n",
      " Bacterial Pneumonia       1689\n",
      "Viral Pneumonia           1182\n",
      "No Pneumonia (healthy)     963\n",
      "COVID-19                   313\n",
      "dtype: int64\n",
      "\n",
      "RESTRICTED TRAINING SAMPLE\n",
      "Number of instances in restricted sample: 1270\n",
      "Instance distribution by class in restricted sample: \n",
      " Bacterial Pneumonia       422\n",
      "COVID-19                  313\n",
      "Viral Pneumonia           295\n",
      "No Pneumonia (healthy)    240\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "if restriction == True:\n",
    "    \n",
    "    ############################\n",
    "    # > RESTRICTION  SUMMARY < #\n",
    "    ############################\n",
    "    print(\"SAMPLE RESTRICTION SUMMARY\")\n",
    "    print(\"\")\n",
    "    print(\"\")\n",
    "    print(\"ORIGINAL TRAINING SAMPLE\")\n",
    "    print(F\"Original number of instances: {len(X_train)}\")\n",
    "    print(f\"Original instance distribution by class: \\n {pd.Series(y_train).value_counts()}\")\n",
    "    print(\"\")\n",
    "    print(\"RESTRICTED TRAINING SAMPLE\")\n",
    "    print(f\"Number of instances in restricted sample: {len(restricted_sample_train)}\")\n",
    "    print(f\"Instance distribution by class in restricted sample: \\n {pd.Series(restricted_sample_labels_train).value_counts()}\")\n",
    "    \n",
    "    X_train = np.array(restricted_sample_train)\n",
    "    y_train = np.array(restricted_sample_labels_train)\n",
    "    X_val = np.array(restricted_sample_val)\n",
    "    y_val = np.array(restricted_sample_labels_val)\n",
    "    X_test = np.array(restricted_sample_test)\n",
    "    y_test = np.array(restricted_sample_labels_test)\n",
    "    \n",
    "else:\n",
    "    print(\"SAMPLE RESTRICTION WAS NOT CONDUCTED\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1270\n",
      "422\n",
      "423\n"
     ]
    }
   ],
   "source": [
    "print(len(X_train))\n",
    "print(len(X_val))\n",
    "print(len(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R-r4ta3MbKLJ"
   },
   "source": [
    "### 1.2 Normalize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "LeplL77mbKS4"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Note from Diego:  discuss in the meeting \n",
    "shall we keep only one code snippet for normalize? Select whichever is the most appropriate\n",
    "'''\n",
    "\n",
    "#making them float \n",
    "X_train=X_train.astype('float32')\n",
    "X_test=X_test.astype('float32')\n",
    "X_val=X_val.astype('float32')\n",
    "\n",
    "#Normalizing the data between 0 and 1 \n",
    "X_train=X_train/255.0\n",
    "X_test=X_test/255.0\n",
    "X_val=X_val/255.0\n",
    "\n",
    "\n",
    "# Compute the mean and standard deviation of the training set\n",
    "train_mean = np.mean(X_train, axis=0)\n",
    "train_std = np.std(X_train, axis=0)\n",
    "\n",
    "# Normalize each set separately using the training set statistics\n",
    "X_train_norm = (X_train - train_mean) / train_std\n",
    "X_val_norm = (X_val - train_mean) / train_std\n",
    "X_test_norm = (X_test - train_mean) / train_std\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Categorical encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a dictionary that maps each category to a numerical value\n",
    "label_map = {\"Bacterial Pneumonia\": 0, \"Viral Pneumonia\": 1, \"No Pneumonia (healthy)\": 2, \"COVID-19\": 3}\n",
    "\n",
    "# Encode the categorical labels as numerical values using the label map\n",
    "y_train_encoded = np.vectorize(label_map.get)(y_train)\n",
    "y_val_encoded = np.vectorize(label_map.get)(y_val)\n",
    "y_test_encoded = np.vectorize(label_map.get)(y_test)\n",
    "\n",
    "# Convert the numerical labels to one-hot encoded format\n",
    "num_classes = 4\n",
    "y_train_onehot = keras.utils.to_categorical(y_train_encoded, num_classes=num_classes)\n",
    "y_val_onehot = keras.utils.to_categorical(y_val_encoded, num_classes=num_classes)\n",
    "y_test_onehot = keras.utils.to_categorical(y_test_encoded, num_classes=num_classes)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YONVgtOAbKca"
   },
   "source": [
    "# 2. Baseline Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Create baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "LhUlV9UNbKiH",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-04 09:02:23.053470: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2023-03-04 09:02:23.055078: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-03-04 09:02:23.056046: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (be39edd05863): /proc/driver/nvidia/version does not exist\n",
      "2023-03-04 09:02:23.064725: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-04 09:02:23.682938: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 370880640 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-04 09:02:27.128376: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 194281472 exceeds 10% of free system memory.\n",
      "2023-03-04 09:02:29.578815: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 194281472 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 1/40 [..............................] - ETA: 4:38 - loss: 1.3875 - accuracy: 0.2500"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-04 09:02:31.498282: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 194281472 exceeds 10% of free system memory.\n",
      "2023-03-04 09:02:33.262060: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 194281472 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40/40 [==============================] - 156s 4s/step - loss: 1.1333 - accuracy: 0.5354 - val_loss: 0.9355 - val_accuracy: 0.6280\n",
      "Epoch 2/10\n",
      "40/40 [==============================] - 147s 4s/step - loss: 0.8364 - accuracy: 0.6630 - val_loss: 0.9171 - val_accuracy: 0.6469\n",
      "Epoch 3/10\n",
      "40/40 [==============================] - 145s 4s/step - loss: 0.7316 - accuracy: 0.7126 - val_loss: 0.8077 - val_accuracy: 0.6801\n",
      "Epoch 4/10\n",
      "40/40 [==============================] - 142s 4s/step - loss: 0.6202 - accuracy: 0.7567 - val_loss: 0.8000 - val_accuracy: 0.6943\n",
      "Epoch 5/10\n",
      "40/40 [==============================] - 133s 3s/step - loss: 0.5285 - accuracy: 0.7882 - val_loss: 0.9739 - val_accuracy: 0.6706\n",
      "Epoch 6/10\n",
      "40/40 [==============================] - 132s 3s/step - loss: 0.5007 - accuracy: 0.7882 - val_loss: 0.7928 - val_accuracy: 0.6777\n",
      "Epoch 7/10\n",
      "40/40 [==============================] - 131s 3s/step - loss: 0.3847 - accuracy: 0.8409 - val_loss: 0.9787 - val_accuracy: 0.6872\n",
      "Epoch 8/10\n",
      "40/40 [==============================] - 130s 3s/step - loss: 0.3533 - accuracy: 0.8638 - val_loss: 1.3406 - val_accuracy: 0.6659\n",
      "Epoch 9/10\n",
      "40/40 [==============================] - 130s 3s/step - loss: 0.2854 - accuracy: 0.8780 - val_loss: 1.4035 - val_accuracy: 0.6682\n",
      "Epoch 10/10\n",
      "40/40 [==============================] - 130s 3s/step - loss: 0.2372 - accuracy: 0.9071 - val_loss: 1.3821 - val_accuracy: 0.6635\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from tensorflow import keras\n",
    "\n",
    "def build_baseline_model():\n",
    "    model = keras.Sequential([\n",
    "        # Convolutional layers\n",
    "        keras.layers.Conv2D(filters=64, kernel_size=(3, 3), activation='relu', input_shape=(156, 156, 3)),\n",
    "        keras.layers.Conv2D(filters=32, kernel_size=(3, 3), activation='relu'),\n",
    "        keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        keras.layers.Conv2D(filters=64, kernel_size=(3, 3), activation='relu'),\n",
    "        keras.layers.Conv2D(filters=32, kernel_size=(3, 3), activation='relu'),\n",
    "        keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        keras.layers.Conv2D(filters=64, kernel_size=(3, 3), activation='relu'),\n",
    "        keras.layers.Conv2D(filters=32, kernel_size=(3, 3), activation='relu'),\n",
    "        keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        \n",
    "        # Dense layers\n",
    "        keras.layers.Flatten(),\n",
    "        keras.layers.Dense(32, activation='relu'),\n",
    "        keras.layers.Dense(32, activation='relu'),\n",
    "        keras.layers.Dense(4, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    # Compile the model with appropriate loss function, optimizer, and metrics\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "baseline_model = build_baseline_model()\n",
    "\n",
    "# Train the model for 10 epochs with a batch size of 32\n",
    "history = baseline_model.fit(\n",
    "    X_train_norm,\n",
    "    y_train_onehot,\n",
    "    batch_size=32,\n",
    "    epochs=10,\n",
    "    validation_data=(X_val_norm, y_val_onehot)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_all_val_acc = np.mean(history.history[\"val_accuracy\"])\n",
    "baseline_all_val_loss = np.mean(history.history[\"val_loss\"])\n",
    "baseline_all_train_acc = np.mean(history.history[\"accuracy\"])\n",
    "baseline_all_train_loss = np.mean(history.history[\"loss\"])\n",
    "print(\"BASELINE RESULTS:\")\n",
    "print(\"-\"*len(\"BASELINE RESULTS:\"))\n",
    "print()\n",
    "print(\"**Training**\")\n",
    "print(\"The average training accuracy among all epochs is: {:.4}\".format(baseline_all_train_acc))\n",
    "print(\"The average training loss among all epochs is: {:.4}\".format(baseline_all_train_loss))\n",
    "print()\n",
    "print(\"**Validation**\")\n",
    "print(\"The average validation accuracy among all epochs is: {:.4}\".format(baseline_all_val_acc))\n",
    "print(\"The average validation loss among all epochs is: {:.4}\".format(baseline_all_val_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zOFuKRShbvTU"
   },
   "source": [
    "### 2.2 Analyze the performance of the baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "58gf79ODcFPD",
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'history2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 6\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackends\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend_pdf\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PdfPages\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m PdfPages(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbaseline_plots.pdf\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m pdf:\n\u001b[1;32m      5\u001b[0m   \u001b[38;5;66;03m##Plot for the accuracy of the baseline model \u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m   accuracy_train \u001b[38;5;241m=\u001b[39m \u001b[43mhistory2\u001b[49m\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      7\u001b[0m   accuracy_val \u001b[38;5;241m=\u001b[39m history2\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      8\u001b[0m   plt\u001b[38;5;241m.\u001b[39mplot(accuracy_train, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtraining_accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'history2' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_acc_loss(histo):\n",
    "  ##Plot for the accuracy of the baseline model \n",
    "  accuracy_train = histo.history['accuracy']\n",
    "  accuracy_val = histo.history['val_accuracy']\n",
    "  plt.plot(accuracy_train, label='training_accuracy')\n",
    "  plt.plot(accuracy_val, label='validation_accuracy')\n",
    "  plt.title('ACCURACY OF THE MODEL')\n",
    "  plt.xlabel('Epochs')\n",
    "  plt.ylabel('Accuracy')\n",
    "  plt.legend()\n",
    "  plt.show()\n",
    "\n",
    "  ##Plot for the loss of the baseline model \n",
    "  loss_train = histo.history['loss']\n",
    "  loss_val = histo.history['val_loss']\n",
    "  plt.plot(loss_train, label='training_loss')\n",
    "  plt.plot(loss_val, label='validation_loss')\n",
    "  plt.title('LOSS OF MODEL')\n",
    "  plt.xlabel('Epochs')\n",
    "  plt.ylabel('Loss')\n",
    "  plt.legend()\n",
    "  plt.show()\n",
    "  return \n",
    "\n",
    "\n",
    "##------ploting from BASELINE only \n",
    "plot_acc_loss(history)\n",
    "\n",
    "\n",
    "##----Getting prediction based in the baseline model \n",
    "y_pred = baseline_model.predict(X_test_norm) \n",
    "\n",
    "def plot_ROC_curve(y_predict,y_test,num_clas): \n",
    "  from sklearn.metrics import roc_curve, auc\n",
    "  import matplotlib.pyplot as plt\n",
    "  fpr = {}\n",
    "  tpr = {}\n",
    "  roc_auc = {}\n",
    "  #calculating roc for each class\n",
    "  for i in range(num_clas):\n",
    "      fpr[i], tpr[i], _ = roc_curve(y_test[:,i], y_predict[:,i])\n",
    "      roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "  from sklearn.metrics import roc_auc_score\n",
    "  # calculating micro-average ROC curve and  area\n",
    "  fpr_micro, tpr_micro, _ = roc_curve(y_test.ravel(), y_predict.ravel())\n",
    "  roc_auc_micro = roc_auc_score(y_test.ravel(), y_predict.ravel())\n",
    "\n",
    "  # Compute macro-average ROC curve and  area\n",
    "  fpr_macro = np.unique(np.concatenate([fpr[i] for i in range(num_clas)]))\n",
    "  tpr_macro = np.zeros_like(fpr_macro)\n",
    "  for i in range(num_clas):\n",
    "      tpr_macro += np.interp(fpr_macro, fpr[i], tpr[i])\n",
    "  tpr_macro /= num_clas\n",
    "  roc_auc_macro = auc(fpr_macro, tpr_macro)\n",
    "\n",
    "  #Plot the ROC curve for each class using matplotlib.pyplot.plot()\n",
    "  plt.figure(figsize=(10, 5))\n",
    "  lw = 2\n",
    "  for i in range(num_clas):\n",
    "      plt.plot(fpr[i], tpr[i], lw=lw, label='ROC curve of class %d (area = %0.2f)' % (i, roc_auc[i]))\n",
    "  plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "  plt.plot(fpr_micro, tpr_micro,lw=lw, linestyle='--', label='micro-average ROC curve (area = %0.2f)' % (roc_auc_micro))\n",
    "  plt.plot(fpr_macro, tpr_macro,lw=lw, linestyle='--', label='macro-average ROC curve (area = %0.2f)' % (roc_auc_macro))\n",
    "  plt.xlim([0.0, 1.0])\n",
    "  plt.ylim([0.0, 1.05])\n",
    "  plt.xlabel('False Positive Rate')\n",
    "  plt.ylabel('True Positive Rate')\n",
    "  plt.title('Receiver Operating Characteristic of Multiclass')\n",
    "  plt.legend(loc=\"lower right\")\n",
    "  plt.show()\n",
    "  return \n",
    "\n",
    "\n",
    "#-------Plot ROC of the baselinemodel \n",
    "plot_ROC_curve(y_pred,y_test_onehot, num_classes)\n",
    "\n",
    "\n",
    "def plot_cm(label_ma,y_predict,y_tes):\n",
    "  #reversing pred to categorical so to get the labels \n",
    "  inverse_label_map = {v: k for k, v in label_ma.items()}  # invert the label_map\n",
    "  y_pred_decoded_numerical = np.argmax(y_predict, axis=1)\n",
    "  y_pred_decoded_categorical = np.vectorize(inverse_label_map.get)(y_pred_decoded_numerical)\n",
    "\n",
    "  #confusion matrix \n",
    "  from sklearn.metrics import confusion_matrix\n",
    "  cm = confusion_matrix(y_tes, y_pred_decoded_categorical)\n",
    "  classes = np.unique(y_tes)\n",
    "  # plot the confusion matrix\n",
    "  fig, ax = plt.subplots()\n",
    "  im = ax.imshow(cm, interpolation='nearest', cmap='Reds')\n",
    "  ax.figure.colorbar(im, ax=ax)\n",
    "  ax.set(xticks=np.arange(cm.shape[1]), yticks=np.arange(cm.shape[0]), xticklabels=classes, yticklabels=classes, ylabel='True label', xlabel='Predicted label')\n",
    "\n",
    "  # rotate the labels\n",
    "  plt.setp(ax.get_xticklabels(), rotation=20, ha=\"right\", rotation_mode=\"anchor\")\n",
    "  # text annotations like the numbers inside \n",
    "  thresh = cm.max() / 2.\n",
    "  for i in range(cm.shape[0]):\n",
    "      for j in range(cm.shape[1]):\n",
    "          ax.text(j, i, format(cm[i, j], 'd'), ha=\"center\", va=\"center\", color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "  plt.show()\n",
    "  return\n",
    "\n",
    "##----CM Matrix plot\n",
    "plot_cm(label_map,y_pred,y_test)\n",
    "\n",
    "\n",
    "def table_p_r_f1(y_tes,y_predict, label_ma):\n",
    "  from sklearn.metrics import classification_report\n",
    "  inverse_label_map = {v: k for k, v in label_ma.items()}  # invert the label_map\n",
    "  y_pred_decoded_numerical = np.argmax(y_predict, axis=1)\n",
    "  y_pred_decoded_categorical = np.vectorize(inverse_label_map.get)(y_pred_decoded_numerical)\n",
    "\n",
    "  print(classification_report(y_test, y_pred_decoded_categorical))\n",
    "\n",
    "#precision and recall and f1-score, accuracy \n",
    "table_p_r_f1(y_test,y_pred,label_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gnxtO6BIb3_P"
   },
   "source": [
    "# 3. Adapting/fine-tuning the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "#             CONTROL PANEL\n",
    "\n",
    "# Sample restriction\n",
    "restriction = True\n",
    "proportion = 0.25\n",
    "\n",
    "# Sample augmentation\n",
    "augmentation_flip = False                       \n",
    "augmentation_rotate = False\n",
    "\n",
    "#                                           \n",
    "#############################################\n",
    "\n",
    "\n",
    "# safety mechanism: augmentation and restriction are not meant to be combined\n",
    "if restriction == True:\n",
    "    agumentation_flip = False\n",
    "    augmentation_rotate = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining some useful functions for restrition/augmentation\n",
    "\n",
    "def collect_labels(data, labels):\n",
    "    \n",
    "    img_label_dict = {}\n",
    "    \n",
    "    # relate image and label index in dictionary\n",
    "    for i in range(len(labels)):\n",
    "        img_label_dict[i] = labels[i]\n",
    "        \n",
    "        \n",
    "    # Store all the indexes by label\n",
    "    BP_index = {k for k,v in img_label_dict.items() if v == 'Bacterial Pneumonia'}\n",
    "    VP_index = {k for k,v in img_label_dict.items() if v == 'Viral Pneumonia'}\n",
    "    NP_index = {k for k,v in img_label_dict.items() if v == 'No Pneumonia (healthy)'}\n",
    "    \n",
    "    CV_index = {k for k,v in img_label_dict.items() if v == 'COVID-19'}\n",
    "\n",
    "    return [BP_index, VP_index, NP_index, CV_index], img_label_dict\n",
    "\n",
    "\n",
    "def restrict_sample(proportion, data, labels):\n",
    "\n",
    "    classified_instances, img_label_dict = collect_labels(X_train, y_train)\n",
    "\n",
    "    restricted_sample_indices = set()\n",
    "    restricted_img_label_dict = {}\n",
    "\n",
    "    for i in classified_instances[:-1]:\n",
    "        tmp = random.sample(i, k=int(len(i)*proportion))\n",
    "        for e in tmp:\n",
    "            restricted_sample_indices.add(e)\n",
    "\n",
    "    restricted_sample_indices = restricted_sample_indices.union(classified_instances[-1])\n",
    "    restricted_sample_labels = [img_label_dict.get(i) for i in restricted_sample_indices]\n",
    "    restricted_sample = [data[i] for i in restricted_sample_indices]\n",
    "\n",
    "    return restricted_sample, restricted_sample_labels\n",
    "    \n",
    "def rotate90(image):\n",
    "    return tf.image.rot90(image)\n",
    "\n",
    "def rotate180(image):\n",
    "    return tf.image.rot90(rotate90(image))\n",
    "\n",
    "def rotate270(image):\n",
    "    return tf.image.rot90(rotate180(image))  \n",
    "\n",
    "\n",
    "def augment_sample(features, labels):\n",
    "\n",
    "    img_label_dict = {}\n",
    "    \n",
    "    # relate image and label index in dictionary\n",
    "    for i in range(len(labels)):\n",
    "        img_label_dict[i] = labels[i]\n",
    "        \n",
    "        \n",
    "    # Store all the indexes by label\n",
    "    BP_index = {k for k,v in img_label_dict.items() if v == 'Bacterial Pneumonia'}\n",
    "    VP_index = {k for k,v in img_label_dict.items() if v == 'Viral Pneumonia'}\n",
    "    NP_index = {k for k,v in img_label_dict.items() if v == 'No Pneumonia (healthy)'}\n",
    "    \n",
    "    CV_index = {k for k,v in img_label_dict.items() if v == 'COVID-19'}\n",
    "    \n",
    "    # Create list with all the previous lists so it can be used to iterate (except covid)\n",
    "    labels_collect = [BP_index, VP_index, NP_index]\n",
    "    \n",
    "    # Select random sample of 50% of each element which is not covid to apply flip transformation\n",
    "    # The remaining 50% stays the same\n",
    "    to_transform = set()\n",
    "    \n",
    "    for i in labels_collect:\n",
    "        tmp = random.sample(i, k=int(len(i)*0.5))\n",
    "        for e in tmp:\n",
    "            to_transform.add(e)\n",
    "            \n",
    "    not_to_transform = set(img_label_dict.keys()) - to_transform - CV_index\n",
    "    # Creating list for keeping track of the labels for the new data        \n",
    "    new_labels = []\n",
    "\n",
    "    for i in to_transform:\n",
    "        new_labels.append(img_label_dict.get(i))\n",
    "    for i in not_to_transform:\n",
    "        new_labels.append(img_label_dict.get(i))\n",
    "    \n",
    "    # Transform the sampled images and include these in the dataset while deleting the original ones\n",
    "    new_features_no_cv = []\n",
    "    \n",
    "    for i in to_transform: # First all transformed instaces are added to the new list\n",
    "            new_features_no_cv.append(tf.image.flip_left_right(features[i]))\n",
    "    for i in not_to_transform: # And then those that will stay the same\n",
    "            new_features_no_cv.append(features[i])\n",
    "    \n",
    "    # Include in new feature set covid x-rays + augmented covid x-rays\n",
    "    new_features_cv = []\n",
    "    \n",
    "    for i in CV_index:\n",
    "        new_features_cv.append(tf.image.flip_left_right(features[i]))\n",
    "        new_features_cv.append(features[i])\n",
    "    \n",
    "    if augmentation_rotate == False:\n",
    "        \n",
    "        new_labels_cv = ['COVID-19' for i in range(len(new_features_cv))]\n",
    "        \n",
    "        augmentedfeatures = new_features_no_cv + new_features_cv\n",
    "        augmentedlabels = new_labels + new_labels_cv\n",
    "        \n",
    "        return augmentedfeatures, augmentedlabels\n",
    "        \n",
    "    # AUGMENTATION - ROTATE    \n",
    "    elif augmentation_rotate == True:\n",
    "        \n",
    "        #Divide no_cv instances in 3 groups (30%, 35%, 35%)\n",
    "        \n",
    "        fd_features, features_1, fd_labels, labels_1 = train_test_split(new_features_no_cv, new_labels, \n",
    "                                                            test_size=0.3, random_state=42)\n",
    "        \n",
    "        features_2, features_3, labels_2, labels_3 = train_test_split(fd_features, fd_labels, \n",
    "                                                            test_size=0.5, random_state=42)\n",
    "        \n",
    "        # Apply rotate90 to the first group and add both original and rotated to the new feature set alongside the labels\n",
    "        \n",
    "        new_features_2 = []\n",
    "        new_labels_2 = []\n",
    "        \n",
    "        for i in range(len(features_1)):\n",
    "            new_features_2.append(features_1[i])\n",
    "            new_features_2.append(rotate90(features_1[i]))\n",
    "            for e in range(2):\n",
    "                new_labels_2.append(labels_1[i])\n",
    "        \n",
    "        # Idem 180\n",
    "        \n",
    "        for i in range(len(features_2)):\n",
    "            new_features_2.append(features_2[i])\n",
    "            new_features_2.append(rotate180(features_2[i]))\n",
    "            for e in range(2):\n",
    "                new_labels_2.append(labels_2[i])\n",
    "        \n",
    "        # Idem 270\n",
    "        \n",
    "        for i in range(len(features_3)):\n",
    "            new_features_2.append(features_3[i])\n",
    "            new_features_2.append(rotate270(features_3[i]))\n",
    "            for e in range(2):\n",
    "                new_labels_2.append(labels_3[i])\n",
    "        \n",
    "        #Apply all rotations to all cv features and add them to the new set alongside the originals and the labels\n",
    "        \n",
    "        for i in range(len(new_features_cv)):\n",
    "            new_features_2.append(new_features_cv[i])\n",
    "            new_features_2.append(rotate90(features_3[i]))\n",
    "            new_features_2.append(rotate180(features_3[i]))\n",
    "            new_features_2.append(rotate270(features_3[i]))\n",
    "            for e in range(4):\n",
    "                new_labels_2.append('COVID-19')\n",
    "        \n",
    "        \n",
    "        augmentedfeatures = new_features_2\n",
    "        augmentedlabels = new_labels_2\n",
    "        \n",
    "        return augmentedfeatures, augmentedlabels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [DISABLED] 3.1 Data restriction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if restriction == True:\n",
    "#\n",
    "#    restricted_sample, restricted_sample_labels = restrict_sample(proportion , X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAMPLE RESTRICTION SUMMARY\n",
      "\n",
      "\n",
      "ORIGINAL SAMPLE\n",
      "Original number of instances: 4147\n",
      "Original instance distribution by class: \n",
      " Bacterial Pneumonia       1689\n",
      "Viral Pneumonia           1182\n",
      "No Pneumonia (healthy)     963\n",
      "COVID-19                   313\n",
      "dtype: int64\n",
      "\n",
      "RESTRICTED SAMPLE\n",
      "Number of instances in restricted sample: 957\n",
      "Instance distribution by class in restricted sample: \n",
      " Bacterial Pneumonia       422\n",
      "COVID-19                  313\n",
      "Viral Pneumonia           295\n",
      "No Pneumonia (healthy)    240\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#if restriction == True:\n",
    "#    \n",
    "#    ############################\n",
    "#    # > RESTRICTION  SUMMARY < #\n",
    "#    ############################\n",
    "#    print(\"SAMPLE RESTRICTION SUMMARY\")\n",
    "#    print(\"\")\n",
    "#    print(\"\")\n",
    "#    print(\"ORIGINAL SAMPLE\")\n",
    "#    print(F\"Original number of instances: {len(X_train)}\")\n",
    "#    print(f\"Original instance distribution by class: \\n {pd.Series(y_train).value_counts()}\")\n",
    "#    print(\"\")\n",
    "#    print(\"RESTRICTED SAMPLE\")\n",
    "#    print(f\"Number of instances in restricted sample: {len(restricted_sample)}\")\n",
    "#    print(f\"Instance distribution by class in restricted sample: \\n {pd.Series(restricted_sample_labels).value_counts()}\")\n",
    "#    \n",
    "#    X_train = np.array(restricted_sample)\n",
    "#    y_train = np.array(restricted_sample_labels)\n",
    "#    \n",
    "#else:\n",
    "#    print(\"SAMPLE RESTRICTION WAS NOT CONDUCTED\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 [DISABLED] Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if augmentation_flip == True:\n",
    "#\n",
    "#    augmentedfeatures, augmentedlabels_ = augment_sample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAMPLE AUGMENTATION WAS NOT CONDUCTED\n"
     ]
    }
   ],
   "source": [
    "#if augmentation_flip == True:\n",
    "#    \n",
    "#    ############################\n",
    "#    # > AUGMENTATION SUMMARY < #\n",
    "#    ############################\n",
    "#    \n",
    "#    print(\"SAMPLE AUGMENTATION SUMMARY\")\n",
    "#    print(\"\")\n",
    "#    print(\"\")\n",
    "#    print(\"ORIGINAL SAMPLE\")\n",
    "#    print(F\"Original number of instances: {len(labels)}\")\n",
    "#    print(f\"Original instance distribution by class: \\n {pd.Series(labels).value_counts()}\")\n",
    "#    print(\"\")\n",
    "#    print(\"AUGMENTED SAMPLE\")\n",
    "#    print(f\"Number of instances in augmented sample: {len(augmentedlabels)}\")\n",
    "#    print(f\"Instance distribution by class in augmented sample: \\n {pd.Series(augmentedlabels).value_counts()}\")\n",
    "#    \n",
    "#    X_train = np.array(augmentedfeatures)\n",
    "#    y_train = np.array(augmentedlabels)\n",
    "#\n",
    "#else:\n",
    "#    print(\"SAMPLE AUGMENTATION WAS NOT CONDUCTED\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 [DISABLED] Categorical encoding of new labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define a dictionary that maps each category to a numerical value\n",
    "#label_map = {\"Bacterial Pneumonia\": 0, \"Viral Pneumonia\": 1, \"No Pneumonia (healthy)\": 2, \"COVID-19\": 3}\n",
    "#\n",
    "## Encode the categorical labels as numerical values using the label map\n",
    "#y_train_encoded = np.vectorize(label_map.get)(y_train)\n",
    "#y_val_encoded = np.vectorize(label_map.get)(y_val)\n",
    "#y_test_encoded = np.vectorize(label_map.get)(y_test)\n",
    "#\n",
    "## Convert the numerical labels to one-hot encoded format\n",
    "#num_classes = 4\n",
    "#y_train_onehot = keras.utils.to_categorical(y_train_encoded, num_classes=num_classes)\n",
    "#y_val_onehot = keras.utils.to_categorical(y_val_encoded, num_classes=num_classes)\n",
    "#y_test_onehot = keras.utils.to_categorical(y_test_encoded, num_classes=num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Network fine tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oiDja0Mub4YW"
   },
   "outputs": [],
   "source": [
    "#After some tryouts, Nadam reported an extra 0-2% val_accuracy in our baseline model vs Adam\n",
    "#So we will find the best learning rate for this optimizer:\n",
    "\n",
    "from tensorflow import keras\n",
    "import keras.backend as K\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def lr_schedule(epoch, initial_lr, final_lr, total_epochs):\n",
    "    \"\"\"\n",
    "    calculates the learning rate for each epoch based on the initial learning rate, final learning rate, and total number of epochs\n",
    "    \"\"\"\n",
    "    lr = initial_lr + (final_lr - initial_lr) * (epoch / float(total_epochs))\n",
    "    return lr\n",
    "\n",
    "def plot_lr_schedule(initial_lr, final_lr, total_epochs):\n",
    "    lr = [lr_schedule(epoch, initial_lr, final_lr, total_epochs) for epoch in range(total_epochs)]\n",
    "    plt.plot(lr, history.history['val_loss'])\n",
    "    plt.xlabel('Learning Rate')\n",
    "    plt.ylabel('Validation Loss')\n",
    "    plt.title('Learning Rate Schedule')\n",
    "    plt.show()\n",
    "\n",
    "# Adding a Lr Scheduler to check the learning rate evolution during training and to avoid overfitting\n",
    "initial_lr = 0.001\n",
    "final_lr = 0.01\n",
    "baseline_epochs = 10\n",
    "\n",
    "lr_scheduler = keras.callbacks.LearningRateScheduler(lambda epoch: lr_schedule(epoch, initial_lr, final_lr, baseline_epochs))\n",
    "\n",
    "baseline_model.compile(loss='binary_crossentropy', optimizer='nadam', metrics=['accuracy'])\n",
    "\n",
    "# Train the model for 4 epochs with a batch size of 32\n",
    "history = baseline_model.fit(\n",
    "    X_train_norm,\n",
    "    y_train_onehot,\n",
    "    batch_size=32,\n",
    "    epochs=baseline_epochs,\n",
    "    validation_data=(X_val_norm, y_val_onehot),\n",
    "    callbacks=[lr_scheduler]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rule of thumb: optimal will be a bit lower than when lr starts climbing, usually 10 times lower the climb up point (around 0.005)\n",
    "plot_lr_schedule(initial_lr, final_lr, baseline_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"from keras import backend as K\n",
    "\n",
    "# Some memory clean-up\n",
    "K.clear_session()\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TUNED MODEL 1: BASELINE + LR SCHEDULER + KFOLD VALIDATION + EARLY STOPPING + LeakyReLU (gridsearch for alpha) + L2 regularizer\n",
    "# FAILED CHANGES VS BASELINE INDICATED WITH A HASHTAG\n",
    "\n",
    "from keras.regularizers import l2\n",
    "\n",
    "def build_tuned_model():\n",
    "    model = keras.Sequential([\n",
    "    # Convolutional layers\n",
    "    keras.layers.Conv2D(filters=64, kernel_size=(3, 3), activation=keras.layers.LeakyReLU(alpha=0.01), input_shape=(156, 156, 3),kernel_regularizer=l2(0.01)),\n",
    "    keras.layers.Conv2D(filters=32, kernel_size=(3, 3),  activation=keras.layers.LeakyReLU(alpha=0.01), kernel_regularizer=l2(0.01)),\n",
    "    keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    keras.layers.Conv2D(filters=64, kernel_size=(3, 3), activation=keras.layers.LeakyReLU(alpha=0.1), kernel_regularizer=l2(0.01)),\n",
    "    keras.layers.Conv2D(filters=32, kernel_size=(3, 3),  activation=keras.layers.LeakyReLU(alpha=0.1), kernel_regularizer=l2(0.01)),\n",
    "    keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    # (CHANGE VS BASELINE) Adding another pack of Conv2D and MaxPooling2D layers\n",
    "    #keras.layers.Conv2D(filters=64, kernel_size=(3, 3), activation='relu'),\n",
    "    #keras.layers.Conv2D(filters=32, kernel_size=(3, 3), activation='relu'),\n",
    "    #keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    keras.layers.Conv2D(filters=64, kernel_size=(3, 3),  activation=keras.layers.LeakyReLU(alpha=0.01), kernel_regularizer=l2(0.01)),\n",
    "    keras.layers.Conv2D(filters=32, kernel_size=(3, 3),  activation=keras.layers.LeakyReLU(alpha=0.01), kernel_regularizer=l2(0.01)),\n",
    "    keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "  \n",
    "    # Dense layers\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(32, activation=keras.layers.LeakyReLU(alpha=0.1), kernel_regularizer=l2(0.01)),\n",
    "    # (CHANGE VS BASELINE) Adding Dropout\n",
    "    #keras.layers.Dropout(0.3),\n",
    "    keras.layers.Dense(32, activation=keras.layers.LeakyReLU(alpha=0.1), kernel_regularizer=l2(0.01)),\n",
    "    # (CHANGE VS BASELINE) Adding an extra dense layer\n",
    "    #keras.layers.Dense(16, activation=keras.layers.LeakyReLU(alpha=0.1)),\n",
    "    # (CHANGE VS BASELINE) Adding Dropout\n",
    "    #keras.layers.Dropout(0.3)\n",
    "    keras.layers.Dense(4, activation='softmax')])\n",
    "    # Compile the model with appropriate loss function, optimizer, and metrics\n",
    "    # (CHANGE VS BASELINE) Adding the optimal lr\n",
    "    optim = keras.optimizers.Nadam(learning_rate=0.001) #(CHANGE VS BASELINE) optimizer = adam, nadam, sgd, rmsprop\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optim, metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "def build_h_model():\n",
    "    '''restnet blocks'''\n",
    "    input_shape = (156, 156, 3)\n",
    "    inputs = keras.Input(shape=input_shape)\n",
    "     # Convolutional layers\n",
    "    x = keras.layers.Conv2D(filters=64, kernel_size=(3, 3), activation=keras.layers.LeakyReLU(alpha=0.01),  padding='same')(inputs)\n",
    "    x = keras.layers.Conv2D(filters=32, kernel_size=(3, 3),  activation=keras.layers.LeakyReLU(alpha=0.01), padding='same')(x)\n",
    "    x = keras.layers.MaxPooling2D(pool_size=(2, 2))(x)\n",
    "\n",
    "    #  block 1\n",
    "    out=x\n",
    "    x = keras.layers.Conv2D(filters=64, kernel_size=(3, 3), activation=keras.layers.LeakyReLU(alpha=0.1), padding='same')(x)\n",
    "    x = keras.layers.Conv2D(filters=32, kernel_size=(3, 3),  activation=keras.layers.LeakyReLU(alpha=0.1), padding='same')(x)\n",
    "    x = keras.layers.add([out, x])\n",
    "    x = keras.layers.LeakyReLU(alpha=0.1)(x)\n",
    "\n",
    "    #  block 2\n",
    "    out=x\n",
    "    x = keras.layers.Conv2D(filters=64, kernel_size=(3, 3), activation=keras.layers.LeakyReLU(alpha=0.1), padding='same')(x)\n",
    "    x = keras.layers.Conv2D(filters=32, kernel_size=(3, 3),  activation=keras.layers.LeakyReLU(alpha=0.1), padding='same')(x)\n",
    "    x = keras.layers.add([out, x])\n",
    "    x = keras.layers.LeakyReLU(alpha=0.1)(x)\n",
    "    #  block 3\n",
    "    out=x\n",
    "    x = keras.layers.Conv2D(filters=64, kernel_size=(3, 3), activation=keras.layers.LeakyReLU(alpha=0.1), padding='same')(x)\n",
    "    x = keras.layers.Conv2D(filters=32, kernel_size=(3, 3),  activation=keras.layers.LeakyReLU(alpha=0.1), padding='same')(x)\n",
    "    x = keras.layers.add([out, x])\n",
    "    x = keras.layers.LeakyReLU(alpha=0.1)(x)\n",
    "    # Dense layers\n",
    "    x = keras.layers.Flatten()(x)\n",
    "    x = keras.layers.Dense(32, activation=keras.layers.LeakyReLU(alpha=0.1))(x)\n",
    "    x = keras.layers.Dense(32, activation=keras.layers.LeakyReLU(alpha=0.1))(x)\n",
    "    outputs = keras.layers.Dense(4, activation='softmax')(x)\n",
    "    optim = keras.optimizers.Nadam(learning_rate=0.001) \n",
    "\n",
    "\n",
    "    # Create model\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs, name='h_model')\n",
    "\n",
    "    # Compile the model with appropriate loss function, optimizer, and metrics\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optim, metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "hybrid_model = build_h_model()\n",
    "\n",
    "# Train the model for 10 epochs with a batch size of 32\n",
    "'''history = hybrid_model.fit(\n",
    "    X_train_norm,\n",
    "    y_train_onehot,\n",
    "    batch_size=32,\n",
    "    epochs=10,\n",
    "    validation_data=(X_val_norm, y_val_onehot))'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K FOLD VALIDATION (5 max epochs for speeding purposes)\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "k = 4\n",
    "num_val_samples = len(X_train_norm) // k \n",
    "num_epochs = 5\n",
    "tuned_all_val_losses = [] # Should add the score of each run at the end of the loop\n",
    "tuned_all_val_acc = []\n",
    "base_all_val_losses = []\n",
    "base_all_val_acc = []\n",
    "res_all_val_acc=[]\n",
    "res_all_val_losses=[]\n",
    "\n",
    "for i in range(k):\n",
    "    print('processing fold #', i)\n",
    "    # Prepare the validation data: data from partition # k\n",
    "    val_data = X_train_norm[i * num_val_samples: (i + 1) * num_val_samples] \n",
    "    val_targets = y_train_onehot[i * num_val_samples: (i + 1) * num_val_samples]\n",
    "\n",
    "    # Prepare the training data: data from all other partitions\n",
    "    partial_train_data = np.concatenate(\n",
    "        [X_train_norm[:i * num_val_samples],\n",
    "         X_train_norm[(i + 1) * num_val_samples:]],\n",
    "        axis=0)\n",
    "    partial_train_targets = np.concatenate(\n",
    "        [y_train_onehot[:i * num_val_samples],\n",
    "         y_train_onehot[(i + 1) * num_val_samples:]],\n",
    "        axis=0)\n",
    "    # (CHANGE VS BASELINE)Defining EarlyStopping callback\n",
    "    early_stopping = keras.callbacks.EarlyStopping(monitor='val_loss', patience=2)\n",
    "\n",
    "    baseline_model = build_baseline_model()\n",
    "    tuned_model_1 = build_tuned_model()\n",
    "    hybrid_model = build_h_model()\n",
    "    \n",
    "\n",
    "    # Train baseline and tuned models for 4 epochs with a batch size of 32\n",
    "    print('processing baseline model')\n",
    "    baseline_history = baseline_model.fit(\n",
    "        partial_train_data,\n",
    "        partial_train_targets,\n",
    "        batch_size=32,\n",
    "        epochs=num_epochs,\n",
    "        validation_data=(val_data, val_targets),\n",
    "        callbacks=[early_stopping]\n",
    "    )\n",
    "    print('processing tuned model')\n",
    "    tuned_history = tuned_model_1.fit(\n",
    "        partial_train_data,\n",
    "        partial_train_targets,\n",
    "        batch_size=32,\n",
    "        epochs=num_epochs,\n",
    "        validation_data=(val_data, val_targets),\n",
    "        callbacks=[early_stopping]\n",
    "    )\n",
    "    print('processing hibrid model 1')\n",
    "    h_history= hybrid_model.fit(\n",
    "        partial_train_data,\n",
    "        partial_train_targets,\n",
    "        batch_size=32,\n",
    "        epochs=num_epochs,\n",
    "        validation_data=(val_data, val_targets),\n",
    "        callbacks=[early_stopping]\n",
    "    )\n",
    "    # Evaluate the kfold results for BASELINE\n",
    "    base_val_loss, base_val_accuracy = baseline_model.evaluate(val_data, val_targets, verbose=0)\n",
    "    base_all_val_losses.append(base_val_loss)\n",
    "    base_all_val_acc.append(base_val_accuracy)\n",
    "\n",
    "    # Evaluate the kfold results for the tuned model\n",
    "    tuned_val_loss, tuned_val_accuracy = tuned_model_1.evaluate(val_data, val_targets, verbose=0)\n",
    "    tuned_all_val_losses.append(tuned_val_loss)\n",
    "    tuned_all_val_acc.append(tuned_val_accuracy)\n",
    "\n",
    "    # Evaluate the kfold results for the hibrid model 1\n",
    "    res_val_loss, res_val_accuracy = hybrid_model.evaluate(val_data, val_targets, verbose=0)\n",
    "    res_all_val_losses.append(res_val_loss)\n",
    "    res_all_val_acc.append(res_val_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Baseline Model KF Results:\")\n",
    "print(\"-\"*len(\"Baseline Model Results:\"))\n",
    "print(\"Avg val_acc: {}\".format(np.mean(base_all_val_acc)))\n",
    "print(\"Avg val_loss: {}\".format(np.mean(base_all_val_losses)))\n",
    "print()\n",
    "print(\"Tuned Model KF Results:\")\n",
    "print(\"-\"*len(\"Tuned Model KF Results:\"))\n",
    "print(\"Avg val_acc: {}\".format(np.mean(tuned_all_val_acc)))\n",
    "print(\"Avg val_loss: {}\".format(np.mean(tuned_all_val_losses)))\n",
    "print()\n",
    "print(\"ResNet Model KF Results:\")\n",
    "print(\"-\"*len(\"ResNetmodel KF Results:\"))\n",
    "print(\"Avg val_acc: {}\".format(np.mean(res_all_val_acc)))\n",
    "print(\"Avg val_loss: {}\".format(np.mean(res_all_val_losses)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_comparison_acc_loss(models_compared,name_models, numepoch):\n",
    "  epoch = range(1, numepoch+1)\n",
    "  clr=['b','r','y','g','k','c']\n",
    "  plt.figure(figsize=(10, 8))\n",
    "\n",
    "  # Plotting the results of validation accuracy and loss for the baseline and tuned models\n",
    "  plt.subplot(2, 2, 1)\n",
    "  i=0\n",
    "  while i<len(models_compared):\n",
    "    plt.plot(epoch, models_compared[i].history['val_accuracy'], clr[i], label=name_models[i])\n",
    "    i+=1\n",
    "  plt.title('Validation accuracy comparison')\n",
    "  plt.xlabel('Epochs')\n",
    "  plt.ylabel('Accuracy')\n",
    "  plt.legend()\n",
    "  \n",
    "  i=0\n",
    "  plt.subplot(2, 2, 2)\n",
    "  while i<len(models_compared):\n",
    "    plt.plot(epoch, models_compared[i].history['val_loss'], clr[i], label=name_models[i])\n",
    "    i+=1\n",
    "  plt.title('Validation loss comparison')\n",
    "  plt.xlabel('Epochs')\n",
    "  plt.ylabel('Loss')\n",
    "  plt.legend()\n",
    "  \n",
    "  i=0\n",
    "  # Plotting the results of training accuracy and loss for the baseline and tuned models  \n",
    "  plt.subplot(2, 2, 3)\n",
    "  while i<len(models_compared):\n",
    "    plt.plot(epoch, models_compared[i].history['accuracy'], clr[i], label=name_models[i])\n",
    "    i+=1\n",
    "  plt.title('Training accuracy comparison')\n",
    "  plt.xlabel('Epochs')\n",
    "  plt.ylabel('Accuracy')\n",
    "  plt.legend()\n",
    "\n",
    "  i=0\n",
    "  plt.subplot(2, 2, 4)\n",
    "  while i<len(models_compared):\n",
    "    plt.plot(epoch, models_compared[i].history['loss'], clr[i], label=name_models[i])\n",
    "    i+=1\n",
    "  plt.title('Training loss comparison')\n",
    "  plt.xlabel('Epochs')\n",
    "  plt.ylabel('Loss')\n",
    "  plt.legend()\n",
    "\n",
    "  plt.subplots_adjust(hspace=0.4,wspace=0.4)\n",
    "  plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "models_compared=[baseline_history,tuned_history,h_history]\n",
    "name_models=['Baseline','Tuned','ResNet']\n",
    "\n",
    "plot_comparison_acc_loss(models_compared,name_models, num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Analyze performance of fine-tuned model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CBGr0x8ZcFn9"
   },
   "source": [
    "# 4. Transfer Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 New data split adapted to transfer learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Transfer learning with VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fn_WnCfDcFyD"
   },
   "outputs": [],
   "source": [
    "# Import VGG16\n",
    "vgg_model = VGG16(include_top=False, input_shape=(156, 156, 3))\n",
    "\n",
    "# FReezing VGG16 layers\n",
    "for layer in vgg_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "## adding \"custom\" layers\n",
    "\n",
    "## Flatten layer\n",
    "flat_1 = layers.Flatten()(vgg_model.layers[-1].output)\n",
    "\n",
    "## Dense layers\n",
    "dense_1 = layers.Dense(32, activation='relu')(flat_1)\n",
    "\n",
    "#output layer with softmax \n",
    "output = layers.Dense(4, activation='softmax')(dense_1)\n",
    "\n",
    "# define new model\n",
    "tl_model = Model(inputs=vgg_model.inputs, outputs=output)\n",
    "\n",
    "# summarize\n",
    "tl_model.summary()\n",
    " \n",
    "# compile model\n",
    "tl_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Early stopping\n",
    "es = EarlyStopping(monitor='val_accuracy', mode='max', patience=5,  restore_best_weights=True)\n",
    "\n",
    "# fit model\n",
    "fitting = tl_model.fit(X_train, y_train_onehot, validation_data=(X_val, y_val_onehot), batch_size=64 ,epochs=25, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Analyze performance of transfer learning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "160d3d50f83ed3d75d0672b6b4d1134bc09c3e08300a8645f571fa0ede095231"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
