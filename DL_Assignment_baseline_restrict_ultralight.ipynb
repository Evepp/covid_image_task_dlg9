{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xveQIshNWYdc"
   },
   "source": [
    "# 0. Imports and data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "I6bqTVRSs-sw",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "All packages and modules are imported in the \"functions.py\" file located next to this notebook\n",
    "\n",
    "Additionally a number of code chunks employed in previous versions of this notebook have been\n",
    "defined into functions and placed in that same file in order to simplify the exposition of the \n",
    "main points and results of the assignment. For further insight on the code behind the actions\n",
    "here executed this \"functions.py\" file can be visited, there all the details can be found.\n",
    "\n",
    "IMPORTANT: for the appropriate functioning of this notebook it must be placed in the same \n",
    "directory as \"functions.py\"\n",
    "'''\n",
    "\n",
    "from functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "b6ZPDhVLWbyq"
   },
   "outputs": [],
   "source": [
    "features = np.load('data/data.npy')\n",
    "labels = np.load('data/labels.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B1b7dP-VbJpC"
   },
   "source": [
    "# 1. Data preprocessing\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Splitting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    features, labels, test_size=0.2, random_state=42, stratify=labels)\n",
    "\n",
    "# Further split the training set into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train, y_train, test_size=0.25, random_state=42, stratify=y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra: Easy going here! - Sample restriction ðŸ˜ŽðŸ¤™ðŸŒ´  \\#prayforkernel ðŸ™"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "#             CONTROL PANEL\n",
    "\n",
    "# Sample restriction\n",
    "restriction = True\n",
    "address_vp_imbalance = True\n",
    "proportion = 0.15\n",
    "#                                           \n",
    "#############################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if restriction == True:\n",
    "    \n",
    "    if address_vp_imbalance == True:\n",
    "        \n",
    "        restricted_sample_train, restricted_sample_labels_train = restrict_sample_vp_imb(proportion , X_train, y_train)\n",
    "        restricted_sample_val, restricted_sample_labels_val = restrict_sample_vp_imb(proportion , X_val, y_val)\n",
    "        restricted_sample_test, restricted_sample_labels_test = restrict_sample_vp_imb(proportion , X_test, y_test)\n",
    "        \n",
    "    else:\n",
    "\n",
    "        restricted_sample_train, restricted_sample_labels_train = restrict_sample(proportion , X_train, y_train)\n",
    "        restricted_sample_val, restricted_sample_labels_val = restrict_sample(proportion , X_val, y_val)\n",
    "        restricted_sample_test, restricted_sample_labels_test = restrict_sample(proportion , X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAMPLE RESTRICTION SUMMARY\n",
      "\n",
      "\n",
      "ORIGINAL TRAINING SAMPLE\n",
      "Original number of instances: 4147\n",
      "Original instance distribution by class: \n",
      " Bacterial Pneumonia       1689\n",
      "Viral Pneumonia           1182\n",
      "No Pneumonia (healthy)     963\n",
      "COVID-19                   313\n",
      "dtype: int64\n",
      "\n",
      "RESTRICTED TRAINING SAMPLE\n",
      "Number of instances in restricted sample: 1047\n",
      "Instance distribution by class in restricted sample: \n",
      " Viral Pneumonia           337\n",
      "COVID-19                  313\n",
      "Bacterial Pneumonia       253\n",
      "No Pneumonia (healthy)    144\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "if restriction == True:\n",
    "    \n",
    "    ############################\n",
    "    # > RESTRICTION  SUMMARY < #\n",
    "    ############################\n",
    "    print(\"SAMPLE RESTRICTION SUMMARY\")\n",
    "    print(\"\")\n",
    "    print(\"\")\n",
    "    print(\"ORIGINAL TRAINING SAMPLE\")\n",
    "    print(F\"Original number of instances: {len(X_train)}\")\n",
    "    print(f\"Original instance distribution by class: \\n {pd.Series(y_train).value_counts()}\")\n",
    "    print(\"\")\n",
    "    print(\"RESTRICTED TRAINING SAMPLE\")\n",
    "    print(f\"Number of instances in restricted sample: {len(restricted_sample_train)}\")\n",
    "    print(f\"Instance distribution by class in restricted sample: \\n {pd.Series(restricted_sample_labels_train).value_counts()}\")\n",
    "    \n",
    "    X_train = np.array(restricted_sample_train)\n",
    "    y_train = np.array(restricted_sample_labels_train)\n",
    "    X_val = np.array(restricted_sample_val)\n",
    "    y_val = np.array(restricted_sample_labels_val)\n",
    "    X_test = np.array(restricted_sample_test)\n",
    "    y_test = np.array(restricted_sample_labels_test)\n",
    "    \n",
    "else:\n",
    "    print(\"SAMPLE RESTRICTION WAS NOT CONDUCTED\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R-r4ta3MbKLJ"
   },
   "source": [
    "### 1.2 Normalize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "LeplL77mbKS4"
   },
   "outputs": [],
   "source": [
    "# Float conversion to allow normalization\n",
    "X_train=X_train.astype('float32')\n",
    "X_test=X_test.astype('float32')\n",
    "X_val=X_val.astype('float32')\n",
    "\n",
    "# Normalization \n",
    "X_train=X_train/255.0\n",
    "X_test=X_test/255.0\n",
    "X_val=X_val/255.0\n",
    "\n",
    "# Compute the mean and standard deviation of the training set for standardization (NOT USED)\n",
    "train_mean = np.mean(X_train, axis=0)\n",
    "train_std = np.std(X_train, axis=0)\n",
    "\n",
    "# Standardization (NOT USED)\n",
    "X_train_norm = (X_train - train_mean) / train_std\n",
    "X_val_norm = (X_val - train_mean) / train_std\n",
    "X_test_norm = (X_test - train_mean) / train_std\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Categorical encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a dictionary that maps each category to a numerical value\n",
    "label_map = {\"Bacterial Pneumonia\": 0, \"Viral Pneumonia\": 1, \"No Pneumonia (healthy)\": 2, \"COVID-19\": 3}\n",
    "\n",
    "# Encode the categorical labels as numerical values using the label map\n",
    "y_train_encoded = np.vectorize(label_map.get)(y_train)\n",
    "y_val_encoded = np.vectorize(label_map.get)(y_val)\n",
    "y_test_encoded = np.vectorize(label_map.get)(y_test)\n",
    "\n",
    "# Convert the numerical labels to one-hot encoded format\n",
    "num_classes = 4\n",
    "y_train_onehot = keras.utils.to_categorical(y_train_encoded, num_classes=num_classes)\n",
    "y_val_onehot = keras.utils.to_categorical(y_val_encoded, num_classes=num_classes)\n",
    "y_test_onehot = keras.utils.to_categorical(y_test_encoded, num_classes=num_classes)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YONVgtOAbKca"
   },
   "source": [
    "# 2. Baseline Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Create baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "LhUlV9UNbKiH",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-04 09:02:23.053470: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2023-03-04 09:02:23.055078: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-03-04 09:02:23.056046: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (be39edd05863): /proc/driver/nvidia/version does not exist\n",
      "2023-03-04 09:02:23.064725: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-04 09:02:23.682938: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 370880640 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-04 09:02:27.128376: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 194281472 exceeds 10% of free system memory.\n",
      "2023-03-04 09:02:29.578815: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 194281472 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 1/40 [..............................] - ETA: 4:38 - loss: 1.3875 - accuracy: 0.2500"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-04 09:02:31.498282: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 194281472 exceeds 10% of free system memory.\n",
      "2023-03-04 09:02:33.262060: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 194281472 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40/40 [==============================] - 156s 4s/step - loss: 1.1333 - accuracy: 0.5354 - val_loss: 0.9355 - val_accuracy: 0.6280\n",
      "Epoch 2/10\n",
      "40/40 [==============================] - 147s 4s/step - loss: 0.8364 - accuracy: 0.6630 - val_loss: 0.9171 - val_accuracy: 0.6469\n",
      "Epoch 3/10\n",
      "40/40 [==============================] - 145s 4s/step - loss: 0.7316 - accuracy: 0.7126 - val_loss: 0.8077 - val_accuracy: 0.6801\n",
      "Epoch 4/10\n",
      "40/40 [==============================] - 142s 4s/step - loss: 0.6202 - accuracy: 0.7567 - val_loss: 0.8000 - val_accuracy: 0.6943\n",
      "Epoch 5/10\n",
      "40/40 [==============================] - 133s 3s/step - loss: 0.5285 - accuracy: 0.7882 - val_loss: 0.9739 - val_accuracy: 0.6706\n",
      "Epoch 6/10\n",
      "40/40 [==============================] - 132s 3s/step - loss: 0.5007 - accuracy: 0.7882 - val_loss: 0.7928 - val_accuracy: 0.6777\n",
      "Epoch 7/10\n",
      "40/40 [==============================] - 131s 3s/step - loss: 0.3847 - accuracy: 0.8409 - val_loss: 0.9787 - val_accuracy: 0.6872\n",
      "Epoch 8/10\n",
      "40/40 [==============================] - 130s 3s/step - loss: 0.3533 - accuracy: 0.8638 - val_loss: 1.3406 - val_accuracy: 0.6659\n",
      "Epoch 9/10\n",
      "40/40 [==============================] - 130s 3s/step - loss: 0.2854 - accuracy: 0.8780 - val_loss: 1.4035 - val_accuracy: 0.6682\n",
      "Epoch 10/10\n",
      "40/40 [==============================] - 130s 3s/step - loss: 0.2372 - accuracy: 0.9071 - val_loss: 1.3821 - val_accuracy: 0.6635\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from tensorflow import keras\n",
    "\n",
    "def build_baseline_model():\n",
    "    model = keras.Sequential([\n",
    "        # Convolutional layers\n",
    "        keras.layers.Conv2D(filters=64, kernel_size=(3, 3), activation='relu', input_shape=(156, 156, 3)),\n",
    "        keras.layers.Conv2D(filters=32, kernel_size=(3, 3), activation='relu'),\n",
    "        keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        keras.layers.Conv2D(filters=64, kernel_size=(3, 3), activation='relu'),\n",
    "        keras.layers.Conv2D(filters=32, kernel_size=(3, 3), activation='relu'),\n",
    "        keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        keras.layers.Conv2D(filters=64, kernel_size=(3, 3), activation='relu'),\n",
    "        keras.layers.Conv2D(filters=32, kernel_size=(3, 3), activation='relu'),\n",
    "        keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        \n",
    "        # Dense layers\n",
    "        keras.layers.Flatten(),\n",
    "        keras.layers.Dense(32, activation='relu'),\n",
    "        keras.layers.Dense(32, activation='relu'),\n",
    "        keras.layers.Dense(4, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    # Compile the model with appropriate loss function, optimizer, and metrics\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "baseline_model = build_baseline_model()\n",
    "\n",
    "# Train the model for 10 epochs with a batch size of 32\n",
    "history = baseline_model.fit(\n",
    "    X_train_norm,\n",
    "    y_train_onehot,\n",
    "    batch_size=32,\n",
    "    epochs=10,\n",
    "    validation_data=(X_val_norm, y_val_onehot)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_all_val_acc = np.mean(history.history[\"val_accuracy\"])\n",
    "baseline_all_val_loss = np.mean(history.history[\"val_loss\"])\n",
    "baseline_all_train_acc = np.mean(history.history[\"accuracy\"])\n",
    "baseline_all_train_loss = np.mean(history.history[\"loss\"])\n",
    "print(\"BASELINE RESULTS:\")\n",
    "print(\"-\"*len(\"BASELINE RESULTS:\"))\n",
    "print()\n",
    "print(\"**Training**\")\n",
    "print(\"The average training accuracy among all epochs is: {:.4}\".format(baseline_all_train_acc))\n",
    "print(\"The average training loss among all epochs is: {:.4}\".format(baseline_all_train_loss))\n",
    "print()\n",
    "print(\"**Validation**\")\n",
    "print(\"The average validation accuracy among all epochs is: {:.4}\".format(baseline_all_val_acc))\n",
    "print(\"The average validation loss among all epochs is: {:.4}\".format(baseline_all_val_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zOFuKRShbvTU"
   },
   "source": [
    "### 2.2 Analyze the performance of the baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "58gf79ODcFPD",
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'history2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 6\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackends\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend_pdf\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PdfPages\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m PdfPages(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbaseline_plots.pdf\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m pdf:\n\u001b[1;32m      5\u001b[0m   \u001b[38;5;66;03m##Plot for the accuracy of the baseline model \u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m   accuracy_train \u001b[38;5;241m=\u001b[39m \u001b[43mhistory2\u001b[49m\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      7\u001b[0m   accuracy_val \u001b[38;5;241m=\u001b[39m history2\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      8\u001b[0m   plt\u001b[38;5;241m.\u001b[39mplot(accuracy_train, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtraining_accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'history2' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_acc_loss(histo):\n",
    "  ##Plot for the accuracy of the baseline model \n",
    "  accuracy_train = histo.history['accuracy']\n",
    "  accuracy_val = histo.history['val_accuracy']\n",
    "  plt.plot(accuracy_train, label='training_accuracy')\n",
    "  plt.plot(accuracy_val, label='validation_accuracy')\n",
    "  plt.title('ACCURACY OF THE MODEL')\n",
    "  plt.xlabel('Epochs')\n",
    "  plt.ylabel('Accuracy')\n",
    "  plt.legend()\n",
    "  plt.show()\n",
    "\n",
    "  ##Plot for the loss of the baseline model \n",
    "  loss_train = histo.history['loss']\n",
    "  loss_val = histo.history['val_loss']\n",
    "  plt.plot(loss_train, label='training_loss')\n",
    "  plt.plot(loss_val, label='validation_loss')\n",
    "  plt.title('LOSS OF MODEL')\n",
    "  plt.xlabel('Epochs')\n",
    "  plt.ylabel('Loss')\n",
    "  plt.legend()\n",
    "  plt.show()\n",
    "  return \n",
    "\n",
    "\n",
    "##------ploting from BASELINE only \n",
    "plot_acc_loss(history)\n",
    "\n",
    "\n",
    "##----Getting prediction based in the baseline model \n",
    "y_pred = baseline_model.predict(X_test_norm) \n",
    "\n",
    "def plot_ROC_curve(y_predict,y_test,num_clas): \n",
    "  from sklearn.metrics import roc_curve, auc\n",
    "  import matplotlib.pyplot as plt\n",
    "  fpr = {}\n",
    "  tpr = {}\n",
    "  roc_auc = {}\n",
    "  #calculating roc for each class\n",
    "  for i in range(num_clas):\n",
    "      fpr[i], tpr[i], _ = roc_curve(y_test[:,i], y_predict[:,i])\n",
    "      roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "  from sklearn.metrics import roc_auc_score\n",
    "  # calculating micro-average ROC curve and  area\n",
    "  fpr_micro, tpr_micro, _ = roc_curve(y_test.ravel(), y_predict.ravel())\n",
    "  roc_auc_micro = roc_auc_score(y_test.ravel(), y_predict.ravel())\n",
    "\n",
    "  # Compute macro-average ROC curve and  area\n",
    "  fpr_macro = np.unique(np.concatenate([fpr[i] for i in range(num_clas)]))\n",
    "  tpr_macro = np.zeros_like(fpr_macro)\n",
    "  for i in range(num_clas):\n",
    "      tpr_macro += np.interp(fpr_macro, fpr[i], tpr[i])\n",
    "  tpr_macro /= num_clas\n",
    "  roc_auc_macro = auc(fpr_macro, tpr_macro)\n",
    "\n",
    "  #Plot the ROC curve for each class using matplotlib.pyplot.plot()\n",
    "  plt.figure(figsize=(10, 5))\n",
    "  lw = 2\n",
    "  for i in range(num_clas):\n",
    "      plt.plot(fpr[i], tpr[i], lw=lw, label='ROC curve of class %d (area = %0.2f)' % (i, roc_auc[i]))\n",
    "  plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "  plt.plot(fpr_micro, tpr_micro,lw=lw, linestyle='--', label='micro-average ROC curve (area = %0.2f)' % (roc_auc_micro))\n",
    "  plt.plot(fpr_macro, tpr_macro,lw=lw, linestyle='--', label='macro-average ROC curve (area = %0.2f)' % (roc_auc_macro))\n",
    "  plt.xlim([0.0, 1.0])\n",
    "  plt.ylim([0.0, 1.05])\n",
    "  plt.xlabel('False Positive Rate')\n",
    "  plt.ylabel('True Positive Rate')\n",
    "  plt.title('Receiver Operating Characteristic of Multiclass')\n",
    "  plt.legend(loc=\"lower right\")\n",
    "  plt.show()\n",
    "  return \n",
    "\n",
    "\n",
    "#-------Plot ROC of the baselinemodel \n",
    "plot_ROC_curve(y_pred,y_test_onehot, num_classes)\n",
    "\n",
    "\n",
    "def plot_cm(label_ma,y_predict,y_tes):\n",
    "  #reversing pred to categorical so to get the labels \n",
    "  inverse_label_map = {v: k for k, v in label_ma.items()}  # invert the label_map\n",
    "  y_pred_decoded_numerical = np.argmax(y_predict, axis=1)\n",
    "  y_pred_decoded_categorical = np.vectorize(inverse_label_map.get)(y_pred_decoded_numerical)\n",
    "\n",
    "  #confusion matrix \n",
    "  from sklearn.metrics import confusion_matrix\n",
    "  cm = confusion_matrix(y_tes, y_pred_decoded_categorical)\n",
    "  classes = np.unique(y_tes)\n",
    "  # plot the confusion matrix\n",
    "  fig, ax = plt.subplots()\n",
    "  im = ax.imshow(cm, interpolation='nearest', cmap='Reds')\n",
    "  ax.figure.colorbar(im, ax=ax)\n",
    "  ax.set(xticks=np.arange(cm.shape[1]), yticks=np.arange(cm.shape[0]), xticklabels=classes, yticklabels=classes, ylabel='True label', xlabel='Predicted label')\n",
    "\n",
    "  # rotate the labels\n",
    "  plt.setp(ax.get_xticklabels(), rotation=20, ha=\"right\", rotation_mode=\"anchor\")\n",
    "  # text annotations like the numbers inside \n",
    "  thresh = cm.max() / 2.\n",
    "  for i in range(cm.shape[0]):\n",
    "      for j in range(cm.shape[1]):\n",
    "          ax.text(j, i, format(cm[i, j], 'd'), ha=\"center\", va=\"center\", color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "  plt.show()\n",
    "  return\n",
    "\n",
    "##----CM Matrix plot\n",
    "plot_cm(label_map,y_pred,y_test)\n",
    "\n",
    "\n",
    "def table_p_r_f1(y_tes,y_predict, label_ma):\n",
    "  from sklearn.metrics import classification_report\n",
    "  inverse_label_map = {v: k for k, v in label_ma.items()}  # invert the label_map\n",
    "  y_pred_decoded_numerical = np.argmax(y_predict, axis=1)\n",
    "  y_pred_decoded_categorical = np.vectorize(inverse_label_map.get)(y_pred_decoded_numerical)\n",
    "\n",
    "  print(classification_report(y_test, y_pred_decoded_categorical))\n",
    "\n",
    "#precision and recall and f1-score, accuracy \n",
    "table_p_r_f1(y_test,y_pred,label_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gnxtO6BIb3_P"
   },
   "source": [
    "# 3. Adapting/fine-tuning the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "#             CONTROL PANEL (Fine tuning)\n",
    "\n",
    "# Sample restriction\n",
    "restriction = True\n",
    "address_vp_imbalance = True\n",
    "proportion = 0.25\n",
    "\n",
    "# Sample augmentation\n",
    "augmentation_flip = True                       \n",
    "augmentation_rotate = True\n",
    "\n",
    "#                                           \n",
    "#############################################\n",
    "\n",
    "\n",
    "# safety mechanism: augmentation and restriction are not meant to be combined\n",
    "if restriction == True:\n",
    "    agumentation_flip = False\n",
    "    augmentation_rotate = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [DISABLED] 3.1 Data restriction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if restriction == True:\n",
    "#\n",
    "#    restricted_sample, restricted_sample_labels = restrict_sample(proportion , X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAMPLE RESTRICTION SUMMARY\n",
      "\n",
      "\n",
      "ORIGINAL SAMPLE\n",
      "Original number of instances: 4147\n",
      "Original instance distribution by class: \n",
      " Bacterial Pneumonia       1689\n",
      "Viral Pneumonia           1182\n",
      "No Pneumonia (healthy)     963\n",
      "COVID-19                   313\n",
      "dtype: int64\n",
      "\n",
      "RESTRICTED SAMPLE\n",
      "Number of instances in restricted sample: 957\n",
      "Instance distribution by class in restricted sample: \n",
      " Bacterial Pneumonia       422\n",
      "COVID-19                  313\n",
      "Viral Pneumonia           295\n",
      "No Pneumonia (healthy)    240\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#if restriction == True:\n",
    "#    \n",
    "#    ############################\n",
    "#    # > RESTRICTION  SUMMARY < #\n",
    "#    ############################\n",
    "#    print(\"SAMPLE RESTRICTION SUMMARY\")\n",
    "#    print(\"\")\n",
    "#    print(\"\")\n",
    "#    print(\"ORIGINAL SAMPLE\")\n",
    "#    print(F\"Original number of instances: {len(X_train)}\")\n",
    "#    print(f\"Original instance distribution by class: \\n {pd.Series(y_train).value_counts()}\")\n",
    "#    print(\"\")\n",
    "#    print(\"RESTRICTED SAMPLE\")\n",
    "#    print(f\"Number of instances in restricted sample: {len(restricted_sample)}\")\n",
    "#    print(f\"Instance distribution by class in restricted sample: \\n {pd.Series(restricted_sample_labels).value_counts()}\")\n",
    "#    \n",
    "#    X_train = np.array(restricted_sample)\n",
    "#    y_train = np.array(restricted_sample_labels)\n",
    "#    \n",
    "#else:\n",
    "#    print(\"SAMPLE RESTRICTION WAS NOT CONDUCTED\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 [DISABLED] Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-06 06:39:41.150174: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2023-03-06 06:39:41.150258: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-03-06 06:39:41.150286: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (be39edd05863): /proc/driver/nvidia/version does not exist\n",
      "2023-03-06 06:39:41.150811: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "#if augmentation_flip == True:\n",
    "#\n",
    "#    augmentedfeatures, augmentedlabels = augment_sample(X_train, y_train, a_rotate=augmentation_rotate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAMPLE AUGMENTATION SUMMARY\n",
      "\n",
      "\n",
      "ORIGINAL SAMPLE\n",
      "Original number of instances: 4147\n",
      "Original instance distribution by class: \n",
      " Bacterial Pneumonia       1689\n",
      "Viral Pneumonia           1182\n",
      "No Pneumonia (healthy)     963\n",
      "COVID-19                   313\n",
      "dtype: int64\n",
      "\n",
      "AUGMENTED SAMPLE\n",
      "Number of instances in augmented sample: 11320\n",
      "Instance distribution by class in augmented sample: \n",
      " Bacterial Pneumonia       3884\n",
      "Viral Pneumonia           2718\n",
      "COVID-19                  2504\n",
      "No Pneumonia (healthy)    2214\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#if augmentation_flip == True:\n",
    "#    \n",
    "#    ############################\n",
    "#    # > AUGMENTATION SUMMARY < #\n",
    "#    ############################\n",
    "#    \n",
    "#    print(\"SAMPLE AUGMENTATION SUMMARY\")\n",
    "#    print(\"\")\n",
    "#    print(\"\")\n",
    "#    print(\"ORIGINAL SAMPLE\")\n",
    "#    print(F\"Original number of instances: {len(y_train)}\")\n",
    "#    print(f\"Original instance distribution by class: \\n {pd.Series(y_train).value_counts()}\")\n",
    "#    print(\"\")\n",
    "#    print(\"AUGMENTED SAMPLE\")\n",
    "#    print(f\"Number of instances in augmented sample: {len(augmentedlabels)}\")\n",
    "#    print(f\"Instance distribution by class in augmented sample: \\n {pd.Series(augmentedlabels).value_counts()}\")\n",
    "#    \n",
    "#    X_train = np.array(augmentedfeatures)\n",
    "#    y_train = np.array(augmentedlabels)\n",
    "#\n",
    "#else:\n",
    "#    print(\"SAMPLE AUGMENTATION WAS NOT CONDUCTED\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 [DISABLED] Categorical encoding of new labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define a dictionary that maps each category to a numerical value\n",
    "#label_map = {\"Bacterial Pneumonia\": 0, \"Viral Pneumonia\": 1, \"No Pneumonia (healthy)\": 2, \"COVID-19\": 3}\n",
    "#\n",
    "## Encode the categorical labels as numerical values using the label map\n",
    "#y_train_encoded = np.vectorize(label_map.get)(y_train)\n",
    "#y_val_encoded = np.vectorize(label_map.get)(y_val)\n",
    "#y_test_encoded = np.vectorize(label_map.get)(y_test)\n",
    "#\n",
    "## Convert the numerical labels to one-hot encoded format\n",
    "#num_classes = 4\n",
    "#y_train_onehot = keras.utils.to_categorical(y_train_encoded, num_classes=num_classes)\n",
    "#y_val_onehot = keras.utils.to_categorical(y_val_encoded, num_classes=num_classes)\n",
    "#y_test_onehot = keras.utils.to_categorical(y_test_encoded, num_classes=num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Network fine tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oiDja0Mub4YW"
   },
   "outputs": [],
   "source": [
    "#After some tryouts, Nadam reported an extra 0-2% val_accuracy in our baseline model vs Adam\n",
    "#So we will find the best learning rate for this optimizer:\n",
    "\n",
    "from tensorflow import keras\n",
    "import keras.backend as K\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def lr_schedule(epoch, initial_lr, final_lr, total_epochs):\n",
    "    \"\"\"\n",
    "    calculates the learning rate for each epoch based on the initial learning rate, final learning rate, and total number of epochs\n",
    "    \"\"\"\n",
    "    lr = initial_lr + (final_lr - initial_lr) * (epoch / float(total_epochs))\n",
    "    return lr\n",
    "\n",
    "def plot_lr_schedule(initial_lr, final_lr, total_epochs):\n",
    "    lr = [lr_schedule(epoch, initial_lr, final_lr, total_epochs) for epoch in range(total_epochs)]\n",
    "    plt.plot(lr, history.history['val_loss'])\n",
    "    plt.xlabel('Learning Rate')\n",
    "    plt.ylabel('Validation Loss')\n",
    "    plt.title('Learning Rate Schedule')\n",
    "    plt.show()\n",
    "\n",
    "# Adding a Lr Scheduler to check the learning rate evolution during training and to avoid overfitting\n",
    "initial_lr = 0.001\n",
    "final_lr = 0.01\n",
    "baseline_epochs = 10\n",
    "\n",
    "lr_scheduler = keras.callbacks.LearningRateScheduler(lambda epoch: lr_schedule(epoch, initial_lr, final_lr, baseline_epochs))\n",
    "\n",
    "baseline_model.compile(loss='binary_crossentropy', optimizer='nadam', metrics=['accuracy'])\n",
    "\n",
    "# Train the model for 4 epochs with a batch size of 32\n",
    "history = baseline_model.fit(\n",
    "    X_train_norm,\n",
    "    y_train_onehot,\n",
    "    batch_size=32,\n",
    "    epochs=baseline_epochs,\n",
    "    validation_data=(X_val_norm, y_val_onehot),\n",
    "    callbacks=[lr_scheduler]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rule of thumb: optimal will be a bit lower than when lr starts climbing, usually 10 times lower the climb up point (around 0.005)\n",
    "plot_lr_schedule(initial_lr, final_lr, baseline_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"from keras import backend as K\n",
    "\n",
    "# Some memory clean-up\n",
    "K.clear_session()\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TUNED MODEL 1: BASELINE + LR SCHEDULER + KFOLD VALIDATION + EARLY STOPPING + LeakyReLU (gridsearch for alpha) + L2 regularizer\n",
    "# FAILED CHANGES VS BASELINE INDICATED WITH A HASHTAG\n",
    "\n",
    "from keras.regularizers import l2\n",
    "\n",
    "def build_tuned_model():\n",
    "    model = keras.Sequential([\n",
    "    # Convolutional layers\n",
    "    keras.layers.Conv2D(filters=64, kernel_size=(3, 3), activation=keras.layers.LeakyReLU(alpha=0.01), input_shape=(156, 156, 3),kernel_regularizer=l2(0.01)),\n",
    "    keras.layers.Conv2D(filters=32, kernel_size=(3, 3),  activation=keras.layers.LeakyReLU(alpha=0.01), kernel_regularizer=l2(0.01)),\n",
    "    keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    keras.layers.Conv2D(filters=64, kernel_size=(3, 3), activation=keras.layers.LeakyReLU(alpha=0.1), kernel_regularizer=l2(0.01)),\n",
    "    keras.layers.Conv2D(filters=32, kernel_size=(3, 3),  activation=keras.layers.LeakyReLU(alpha=0.1), kernel_regularizer=l2(0.01)),\n",
    "    keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    # (CHANGE VS BASELINE) Adding another pack of Conv2D and MaxPooling2D layers\n",
    "    #keras.layers.Conv2D(filters=64, kernel_size=(3, 3), activation='relu'),\n",
    "    #keras.layers.Conv2D(filters=32, kernel_size=(3, 3), activation='relu'),\n",
    "    #keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    keras.layers.Conv2D(filters=64, kernel_size=(3, 3),  activation=keras.layers.LeakyReLU(alpha=0.01), kernel_regularizer=l2(0.01)),\n",
    "    keras.layers.Conv2D(filters=32, kernel_size=(3, 3),  activation=keras.layers.LeakyReLU(alpha=0.01), kernel_regularizer=l2(0.01)),\n",
    "    keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "  \n",
    "    # Dense layers\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(32, activation=keras.layers.LeakyReLU(alpha=0.1), kernel_regularizer=l2(0.01)),\n",
    "    # (CHANGE VS BASELINE) Adding Dropout\n",
    "    #keras.layers.Dropout(0.3),\n",
    "    keras.layers.Dense(32, activation=keras.layers.LeakyReLU(alpha=0.1), kernel_regularizer=l2(0.01)),\n",
    "    # (CHANGE VS BASELINE) Adding an extra dense layer\n",
    "    #keras.layers.Dense(16, activation=keras.layers.LeakyReLU(alpha=0.1)),\n",
    "    # (CHANGE VS BASELINE) Adding Dropout\n",
    "    #keras.layers.Dropout(0.3)\n",
    "    keras.layers.Dense(4, activation='softmax')])\n",
    "    # Compile the model with appropriate loss function, optimizer, and metrics\n",
    "    # (CHANGE VS BASELINE) Adding the optimal lr\n",
    "    optim = keras.optimizers.Nadam(learning_rate=0.001) #(CHANGE VS BASELINE) optimizer = adam, nadam, sgd, rmsprop\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optim, metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "def build_h_model():\n",
    "    '''restnet blocks'''\n",
    "    input_shape = (156, 156, 3)\n",
    "    inputs = keras.Input(shape=input_shape)\n",
    "     # Convolutional layers\n",
    "    x = keras.layers.Conv2D(filters=64, kernel_size=(3, 3), activation=keras.layers.LeakyReLU(alpha=0.01),  padding='same')(inputs)\n",
    "    x = keras.layers.Conv2D(filters=32, kernel_size=(3, 3),  activation=keras.layers.LeakyReLU(alpha=0.01), padding='same')(x)\n",
    "    x = keras.layers.MaxPooling2D(pool_size=(2, 2))(x)\n",
    "\n",
    "    #  block 1\n",
    "    out=x\n",
    "    x = keras.layers.Conv2D(filters=64, kernel_size=(3, 3), activation=keras.layers.LeakyReLU(alpha=0.1), padding='same')(x)\n",
    "    x = keras.layers.Conv2D(filters=32, kernel_size=(3, 3),  activation=keras.layers.LeakyReLU(alpha=0.1), padding='same')(x)\n",
    "    x = keras.layers.add([out, x])\n",
    "    x = keras.layers.LeakyReLU(alpha=0.1)(x)\n",
    "\n",
    "    #  block 2\n",
    "    out=x\n",
    "    x = keras.layers.Conv2D(filters=64, kernel_size=(3, 3), activation=keras.layers.LeakyReLU(alpha=0.1), padding='same')(x)\n",
    "    x = keras.layers.Conv2D(filters=32, kernel_size=(3, 3),  activation=keras.layers.LeakyReLU(alpha=0.1), padding='same')(x)\n",
    "    x = keras.layers.add([out, x])\n",
    "    x = keras.layers.LeakyReLU(alpha=0.1)(x)\n",
    "    #  block 3\n",
    "    out=x\n",
    "    x = keras.layers.Conv2D(filters=64, kernel_size=(3, 3), activation=keras.layers.LeakyReLU(alpha=0.1), padding='same')(x)\n",
    "    x = keras.layers.Conv2D(filters=32, kernel_size=(3, 3),  activation=keras.layers.LeakyReLU(alpha=0.1), padding='same')(x)\n",
    "    x = keras.layers.add([out, x])\n",
    "    x = keras.layers.LeakyReLU(alpha=0.1)(x)\n",
    "    # Dense layers\n",
    "    x = keras.layers.Flatten()(x)\n",
    "    x = keras.layers.Dense(32, activation=keras.layers.LeakyReLU(alpha=0.1))(x)\n",
    "    x = keras.layers.Dense(32, activation=keras.layers.LeakyReLU(alpha=0.1))(x)\n",
    "    outputs = keras.layers.Dense(4, activation='softmax')(x)\n",
    "    optim = keras.optimizers.Nadam(learning_rate=0.001) \n",
    "\n",
    "\n",
    "    # Create model\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs, name='h_model')\n",
    "\n",
    "    # Compile the model with appropriate loss function, optimizer, and metrics\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optim, metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "hybrid_model = build_h_model()\n",
    "\n",
    "# Train the model for 10 epochs with a batch size of 32\n",
    "'''history = hybrid_model.fit(\n",
    "    X_train_norm,\n",
    "    y_train_onehot,\n",
    "    batch_size=32,\n",
    "    epochs=10,\n",
    "    validation_data=(X_val_norm, y_val_onehot))'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K FOLD VALIDATION (5 max epochs for speeding purposes)\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "k = 4\n",
    "num_val_samples = len(X_train_norm) // k \n",
    "num_epochs = 5\n",
    "tuned_all_val_losses = [] # Should add the score of each run at the end of the loop\n",
    "tuned_all_val_acc = []\n",
    "base_all_val_losses = []\n",
    "base_all_val_acc = []\n",
    "res_all_val_acc=[]\n",
    "res_all_val_losses=[]\n",
    "\n",
    "for i in range(k):\n",
    "    print('processing fold #', i)\n",
    "    # Prepare the validation data: data from partition # k\n",
    "    val_data = X_train_norm[i * num_val_samples: (i + 1) * num_val_samples] \n",
    "    val_targets = y_train_onehot[i * num_val_samples: (i + 1) * num_val_samples]\n",
    "\n",
    "    # Prepare the training data: data from all other partitions\n",
    "    partial_train_data = np.concatenate(\n",
    "        [X_train_norm[:i * num_val_samples],\n",
    "         X_train_norm[(i + 1) * num_val_samples:]],\n",
    "        axis=0)\n",
    "    partial_train_targets = np.concatenate(\n",
    "        [y_train_onehot[:i * num_val_samples],\n",
    "         y_train_onehot[(i + 1) * num_val_samples:]],\n",
    "        axis=0)\n",
    "    # (CHANGE VS BASELINE)Defining EarlyStopping callback\n",
    "    early_stopping = keras.callbacks.EarlyStopping(monitor='val_loss', patience=2)\n",
    "\n",
    "    baseline_model = build_baseline_model()\n",
    "    tuned_model_1 = build_tuned_model()\n",
    "    hybrid_model = build_h_model()\n",
    "    \n",
    "\n",
    "    # Train baseline and tuned models for 4 epochs with a batch size of 32\n",
    "    print('processing baseline model')\n",
    "    baseline_history = baseline_model.fit(\n",
    "        partial_train_data,\n",
    "        partial_train_targets,\n",
    "        batch_size=32,\n",
    "        epochs=num_epochs,\n",
    "        validation_data=(val_data, val_targets),\n",
    "        callbacks=[early_stopping]\n",
    "    )\n",
    "    print('processing tuned model')\n",
    "    tuned_history = tuned_model_1.fit(\n",
    "        partial_train_data,\n",
    "        partial_train_targets,\n",
    "        batch_size=32,\n",
    "        epochs=num_epochs,\n",
    "        validation_data=(val_data, val_targets),\n",
    "        callbacks=[early_stopping]\n",
    "    )\n",
    "    print('processing hibrid model 1')\n",
    "    h_history= hybrid_model.fit(\n",
    "        partial_train_data,\n",
    "        partial_train_targets,\n",
    "        batch_size=32,\n",
    "        epochs=num_epochs,\n",
    "        validation_data=(val_data, val_targets),\n",
    "        callbacks=[early_stopping]\n",
    "    )\n",
    "    # Evaluate the kfold results for BASELINE\n",
    "    base_val_loss, base_val_accuracy = baseline_model.evaluate(val_data, val_targets, verbose=0)\n",
    "    base_all_val_losses.append(base_val_loss)\n",
    "    base_all_val_acc.append(base_val_accuracy)\n",
    "\n",
    "    # Evaluate the kfold results for the tuned model\n",
    "    tuned_val_loss, tuned_val_accuracy = tuned_model_1.evaluate(val_data, val_targets, verbose=0)\n",
    "    tuned_all_val_losses.append(tuned_val_loss)\n",
    "    tuned_all_val_acc.append(tuned_val_accuracy)\n",
    "\n",
    "    # Evaluate the kfold results for the hibrid model 1\n",
    "    res_val_loss, res_val_accuracy = hybrid_model.evaluate(val_data, val_targets, verbose=0)\n",
    "    res_all_val_losses.append(res_val_loss)\n",
    "    res_all_val_acc.append(res_val_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Baseline Model KF Results:\")\n",
    "print(\"-\"*len(\"Baseline Model Results:\"))\n",
    "print(\"Avg val_acc: {}\".format(np.mean(base_all_val_acc)))\n",
    "print(\"Avg val_loss: {}\".format(np.mean(base_all_val_losses)))\n",
    "print()\n",
    "print(\"Tuned Model KF Results:\")\n",
    "print(\"-\"*len(\"Tuned Model KF Results:\"))\n",
    "print(\"Avg val_acc: {}\".format(np.mean(tuned_all_val_acc)))\n",
    "print(\"Avg val_loss: {}\".format(np.mean(tuned_all_val_losses)))\n",
    "print()\n",
    "print(\"ResNet Model KF Results:\")\n",
    "print(\"-\"*len(\"ResNetmodel KF Results:\"))\n",
    "print(\"Avg val_acc: {}\".format(np.mean(res_all_val_acc)))\n",
    "print(\"Avg val_loss: {}\".format(np.mean(res_all_val_losses)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_comparison_acc_loss(models_compared,name_models, numepoch):\n",
    "  epoch = range(1, numepoch+1)\n",
    "  clr=['b','r','y','g','k','c']\n",
    "  plt.figure(figsize=(10, 8))\n",
    "\n",
    "  # Plotting the results of validation accuracy and loss for the baseline and tuned models\n",
    "  plt.subplot(2, 2, 1)\n",
    "  i=0\n",
    "  while i<len(models_compared):\n",
    "    plt.plot(epoch, models_compared[i].history['val_accuracy'], clr[i], label=name_models[i])\n",
    "    i+=1\n",
    "  plt.title('Validation accuracy comparison')\n",
    "  plt.xlabel('Epochs')\n",
    "  plt.ylabel('Accuracy')\n",
    "  plt.legend()\n",
    "  \n",
    "  i=0\n",
    "  plt.subplot(2, 2, 2)\n",
    "  while i<len(models_compared):\n",
    "    plt.plot(epoch, models_compared[i].history['val_loss'], clr[i], label=name_models[i])\n",
    "    i+=1\n",
    "  plt.title('Validation loss comparison')\n",
    "  plt.xlabel('Epochs')\n",
    "  plt.ylabel('Loss')\n",
    "  plt.legend()\n",
    "  \n",
    "  i=0\n",
    "  # Plotting the results of training accuracy and loss for the baseline and tuned models  \n",
    "  plt.subplot(2, 2, 3)\n",
    "  while i<len(models_compared):\n",
    "    plt.plot(epoch, models_compared[i].history['accuracy'], clr[i], label=name_models[i])\n",
    "    i+=1\n",
    "  plt.title('Training accuracy comparison')\n",
    "  plt.xlabel('Epochs')\n",
    "  plt.ylabel('Accuracy')\n",
    "  plt.legend()\n",
    "\n",
    "  i=0\n",
    "  plt.subplot(2, 2, 4)\n",
    "  while i<len(models_compared):\n",
    "    plt.plot(epoch, models_compared[i].history['loss'], clr[i], label=name_models[i])\n",
    "    i+=1\n",
    "  plt.title('Training loss comparison')\n",
    "  plt.xlabel('Epochs')\n",
    "  plt.ylabel('Loss')\n",
    "  plt.legend()\n",
    "\n",
    "  plt.subplots_adjust(hspace=0.4,wspace=0.4)\n",
    "  plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "models_compared=[baseline_history,tuned_history,h_history]\n",
    "name_models=['Baseline','Tuned','ResNet']\n",
    "\n",
    "plot_comparison_acc_loss(models_compared,name_models, num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Analyze performance of fine-tuned model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CBGr0x8ZcFn9"
   },
   "source": [
    "# 4. Transfer Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 New data split and subsequent preprocessing adapted to transfer learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    features, labels, test_size=0.2, random_state=42, stratify=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "#             CONTROL PANEL (Transfer Learning)\n",
    "\n",
    "# Sample restriction\n",
    "restriction = True\n",
    "address_vp_imbalance = True\n",
    "proportion = 0.25\n",
    "\n",
    "# Sample augmentation\n",
    "augmentation_flip = True                       \n",
    "augmentation_rotate = True\n",
    "\n",
    "#                                           \n",
    "#############################################\n",
    "\n",
    "\n",
    "# safety mechanism: augmentation and restriction are not meant to be combined\n",
    "if restriction == True:\n",
    "    agumentation_flip = False\n",
    "    augmentation_rotate = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if restriction == True:\n",
    "    \n",
    "    if address_vp_imbalance == True:\n",
    "        restricted_sample_train, restricted_sample_labels_train = restrict_sample_vp_imb(proportion , X_train, y_train)\n",
    "        \n",
    "    else:\n",
    "        restricted_sample_train, restricted_sample_labels_train = restrict_sample(proportion , X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=X_train.astype('float32')\n",
    "X_val=X_val.astype('float32')\n",
    "\n",
    "# Normalization \n",
    "X_train=X_train/255.0\n",
    "X_val=X_val/255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if restriction == True:\n",
    "    \n",
    "    ############################\n",
    "    # > RESTRICTION  SUMMARY < #\n",
    "    ############################\n",
    "    print(\"SAMPLE RESTRICTION SUMMARY\")\n",
    "    print(\"\")\n",
    "    print(\"\")\n",
    "    print(\"ORIGINAL TRAINING SAMPLE\")\n",
    "    print(F\"Original number of instances: {len(X_train)}\")\n",
    "    print(f\"Original instance distribution by class: \\n {pd.Series(y_train).value_counts()}\")\n",
    "    print(\"\")\n",
    "    print(\"RESTRICTED TRAINING SAMPLE\")\n",
    "    print(f\"Number of instances in restricted sample: {len(restricted_sample_train)}\")\n",
    "    print(f\"Instance distribution by class in restricted sample: \\n {pd.Series(restricted_sample_labels_train).value_counts()}\")\n",
    "    \n",
    "    X_train = np.array(restricted_sample_train)\n",
    "    y_train = np.array(restricted_sample_labels_train)\n",
    "    X_val = np.array(restricted_sample_val)\n",
    "    y_val = np.array(restricted_sample_labels_val)\n",
    "    X_test = np.array(restricted_sample_test)\n",
    "    y_test = np.array(restricted_sample_labels_test)\n",
    "    \n",
    "else:\n",
    "    print(\"SAMPLE RESTRICTION WAS NOT CONDUCTED\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Transfer learning with VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fn_WnCfDcFyD"
   },
   "outputs": [],
   "source": [
    "# Import VGG16\n",
    "vgg_model = VGG16(include_top=False, input_shape=(156, 156, 3))\n",
    "\n",
    "# FReezing VGG16 layers\n",
    "for layer in vgg_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "## adding \"custom\" layers\n",
    "\n",
    "## Flatten layer\n",
    "flat_1 = layers.Flatten()(vgg_model.layers[-1].output)\n",
    "\n",
    "## Dense layers\n",
    "dense_1 = layers.Dense(32, activation='relu')(flat_1)\n",
    "\n",
    "#output layer with softmax \n",
    "output = layers.Dense(4, activation='softmax')(dense_1)\n",
    "\n",
    "# define new model\n",
    "tl_model = Model(inputs=vgg_model.inputs, outputs=output)\n",
    "\n",
    "# summarize\n",
    "tl_model.summary()\n",
    " \n",
    "# compile model\n",
    "tl_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Early stopping\n",
    "es = EarlyStopping(monitor='val_accuracy', mode='max', patience=5,  restore_best_weights=True)\n",
    "\n",
    "# fit model\n",
    "fitting = tl_model.fit(X_train, y_train_onehot, validation_data=(X_val, y_val_onehot), batch_size=64 ,epochs=25, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Analyze performance of transfer learning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "160d3d50f83ed3d75d0672b6b4d1134bc09c3e08300a8645f571fa0ede095231"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
